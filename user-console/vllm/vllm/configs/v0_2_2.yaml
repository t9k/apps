## @param image.server.registry Repository registry for the server image
## @param image.server.repository Repository name for the server image
## @param image.server.tag Image tag for the server image (immutable tags are recommended)
## @param image.initializer.registry Repository registry for the initializer image
## @param image.initializer.repository Repository name for the initializer image
## @param image.initializer.tag Image tag for the initializer image (immutable tags are recommended)
##
image:
  # 推理服务器镜像
  server:
    registry: "$(T9K_APP_IMAGE_REGISTRY)"
    repository: "$(T9K_APP_IMAGE_NAMESPACE)/vllm-openai"
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v0.6.3"

  # 初始化器镜像，请勿修改
  initializer:
    registry: "$(T9K_APP_IMAGE_REGISTRY)"
    repository: "$(T9K_APP_IMAGE_NAMESPACE)/kubectl"
    tag: "1.27"

## @param resources.limits.cpu CPU limit for Kubernetes resources
## @param resources.limits.memory Memory limit for Kubernetes resources
## @param resources.limits."nvidia.com/gpu" NVIDIA GPU limit for Kubernetes resources
##
resources:
  limits:
    cpu: 4
    memory: 64Gi
    nvidia.com/gpu: 1

## @param model.name Name by which the model is deployed
## @param model.volume.storageClass Storage class for the model volume
## @param model.volume.size Size of the model volume
## @param model.volume.accessModes Access modes for the model volume
## @param model.volume.existingClaim PVC for the model volume, if bringing your own PVC
## @param model.volume.subPath Subdirectory of the model volume to mount
## @param model.source Source of the model (`huggingface`, `git`, `s3`, or "")
## @param model.huggingface.id Hugging Face ID of the model
## @param model.huggingface.files Files to download from the Hugging Face model repository
## @param model.huggingface.existingSecret Secret whose value of key `token` is used as the Hugging Face token
## @param model.git.url Git repository URL for the model
## @param model.git.ref Git branch, tag, or commit for the model
## @param model.git.existingSecret Secret whose value of key `token` is used as the Git token
## @param model.s3.url S3 URL for the model
## @param model.s3.existingSecret Secret of type s3-env containing the S3 credentials
##
model:
  # 模型以该名称被部署
  deployName: ""

  volume:
    storageClass: ""
    size: 32Gi
    accessModes:
      - ReadWriteOnce
    existingClaim: ""
    subPath: ""

  # 模型来源
  # 应为 ""、"huggingface"、"git" 或 "s3"，"" 表示不下载
  source: ""
  huggingface:
    # 模型的 Hugging Face ID，例如 "meta-llama/Meta-Llama-3-8B"
    id: ""
    # 下载的文件列表，例如 "README.md,tokenizer.json,tokenizer_config.json"，默认为所有文件
    files: ""
    # 引用 Secret 的键 token 的值作为 Hugging Face token
    existingSecret: ""
  git:
    # Git 仓库路径
    url: ""
    # 分支、标签或 commit
    ref: ""
    # 引用 Secret 的键 token 的值作为 Git token
    existingSecret: ""
  s3:
    # S3 路径
    url: ""
    # 引用 s3-env 类型的 Secret 的键值
    existingSecret: ""

## @param autoScaling.minReplicas Minimum number of replicas for autoscaling
## @param autoScaling.maxReplicas Maximum number of replicas for autoscaling
## @param autoScaling.annotations Annotations for Knative Service autoscalin
##
autoScaling:
  minReplicas: 1
  maxReplicas: 1

  # 为 Knative Service 设置的注解，参考 https://knative.dev/docs/serving/autoscaling/autoscaling-metrics/
  annotations: {}
    # autoscaling.knative.dev/metric: "concurrency"
    # autoscaling.knative.dev/target-utilization-percentage: "70" 

## @param app.extraArgs Extra arguments for the vLLM engine
##
app:
  # 为 vLLM engine 设置的额外参数，参考 https://docs.vllm.ai/en/latest/models/engine_args.html
  # 请勿设置 --model, --served-model-name, --tensor-parallel-size
  extraArgs: []
    # - "--trust-remote-code"
    # - "--enforce-eager"
    # - "--gpu-memory-utilization=1.0"
    # - "--max-model-len=8192"
    # - "--quantization=bitsandbytes"
    # - "--load-format=bitsandbytes"

## @param env Array of extra environment variables
##
env: []
  # # HTTP 和 HTTPS 代理
  # - name: HTTP_PROXY
  #   value: ""
  # - name: HTTPS_PROXY
  #   value: ""
