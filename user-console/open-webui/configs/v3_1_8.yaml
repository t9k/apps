## @param image.registry Open WebUI image registry
## @param image.repository Open WebUI image repository
## @param image.tag Open WebUI image tag
## @param image.pullPolicy Open WebUI image pull policy
##
image:
  registry: "$(T9K_APP_IMAGE_REGISTRY)"
  repository: "$(T9K_APP_IMAGE_NAMESPACE)/open-webui"
  tag: "git-96c8654"
  pullPolicy: IfNotPresent

## @param resources.limits.cpu CPU limit for Open WebUI container
## @param resources.limits.memory Memory limit for Open WebUI container
##
resources:
  limits:
    cpu: 1
    memory: 2Gi

## @param persistence.size Size of the PVC
## @param persistence.storageClass Storage class for the PVC
## @param persistence.accessModes Access modes for the PVC. If using multiple replicas, you must update accessModes to ReadWriteMany
## @param persistence.existingClaim Name of an existing PVC to use
##
persistence:
  size: 2Gi
  storageClass: ""
  accessModes:
    - ReadWriteOnce
  existingClaim: ""

## @param ollama.enabled Enable/disable installation of Ollama Helm chart
## @param ollama.ollama.gpu.enabled Enable/disable GPU support for Ollama
## @param ollama.ollama.gpu.type GPU type for Ollama (`nvidia` or `amd`)
## @param ollama.ollama.gpu.number Number of GPUs to allocate for Ollama
## @param ollama.ollama.models List of models to be deployed with Ollama
## @param ollama.image.registry Ollama image registry
## @param ollama.image.repository Ollama image repository
## @param ollama.image.tag Ollama image tag
## @param ollama.image.pullPolicy Ollama image pull Policy
## @param ollama.resources.limits.cpu CPU limit for Ollama server container
## @param ollama.resources.limits.memory Memory limit for Ollama server container
## @param ollama.persistentVolume.enabled Enable persistence using PVC
## @param ollama.persistentVolume.size Size of the PVC for Ollama server data
##
ollama:
  enabled: true

  ollama:
    gpu:
      enabled: true
      type: 'nvidia'
      number: 1
    models: []
    #  - llama3
    #  - mistral
  image:
    registry: "$(T9K_APP_IMAGE_REGISTRY)"
    repository: "$(T9K_APP_IMAGE_NAMESPACE)/ollama"
    tag: "0.3.6"
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 2
      # cpu: 16  # if gpu is disabled
      memory: 16Gi

  persistentVolume:
    enabled: true
    size: 30Gi

## @param ollamaUrls A list of Ollama API endpoints. These can be added in lieu of automatically installing the Ollama Helm chart, or in addition to it.
##
ollamaUrls: []

## @param pipelines.enabled Automatically install Pipelines chart to extend Open WebUI functionality using Pipelines: https://github.com/open-webui/pipelines
## @param pipelines.extraEnvVars This section can be used to pass required environment variables to your pipelines (e.g. Langfuse hostname)
## @param pipelines.image.registry Pipelines image registry
## @param pipelines.image.repository Pipelines image repository
## @param pipelines.image.tag Pipelines image tag
## @param pipelines.image.pullPolicy Pipelines image pull Policy
## @param pipelines.resources.limits.cpu CPU limit for Pipelines server container
## @param pipelines.resources.limits.memory Memory limit for Pipelines server container
##
pipelines:
  enabled: true
  extraEnvVars: []
  image:
    registry: "$(T9K_APP_IMAGE_REGISTRY)"
    repository: "$(T9K_APP_IMAGE_NAMESPACE)/pipelines"
    tag: main
    pullPolicy: Always
  resources:
    limits:
      cpu: 4
      memory: 8Gi

## @param env Array of extra environment variables
##
env: []
  # - name: OPENAI_API_KEY
  #   valueFrom:
  #     secretKeyRef:
  #       name: openai-api-key
  #       key: api-key
  # - name: OLLAMA_DEBUG
  #   value: "1"
