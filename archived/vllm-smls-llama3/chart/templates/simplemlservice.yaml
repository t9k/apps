apiVersion: tensorstack.dev/v1beta1
kind: SimpleMLService
metadata:
  name: {{ include "vllm.fullname" . }}
  labels:
    {{- include "vllm.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  storage:
    pvc:
      name: {{ .Values.model.existingClaim }}
      subPath: {{ .Values.model.subPath }}
  service:
    type: ClusterIP
    ports:
    - name: http
      port: 8000
      targetPort: 8000
      protocol: TCP
  custom:
    spec:
      containers:
      - name: server
        image: "{{ .Values.server.image.registry }}/{{ .Values.server.image.repository }}:{{ .Values.server.image.tag }}"
        imagePullPolicy: {{ .Values.imagePullPolicy }}
        args:
          - --model=/var/lib/t9k/model
          - --served-model-name={{ .Values.model.deployName }}
          - --tensor-parallel-size={{ (index .Values.resources.limits "nvidia.com/gpu") }}
          {{ range .Values.app.extraArgs }}
          - {{ . }}
          {{ end }}
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 15
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          periodSeconds: 15
          failureThreshold: 5
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
        volumeMounts:
        - name: dshm
          mountPath: /dev/shm
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: "64Gi"
